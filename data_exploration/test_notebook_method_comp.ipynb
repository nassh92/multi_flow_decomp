{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36eabb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Baseline Heuristics\n",
    "\"\"\"\n",
    "# Import pickle\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import pylab as py\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import sys\n",
    "\n",
    "import os\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\HADDAM\\\\Documents\\\\Python Scripts\\\\multi_flow_decomp\\\\')\n",
    "\n",
    "from utils.utils_stats import mean_confidence_interval\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "{\"res_key_metadata\":res_key_metadata,\n",
    "\"res_value_metadata\":res_value_metadata, \n",
    "\"data\":deepcopy(dict(dict_results))}\n",
    "\"\"\"\n",
    "simulated_instances = False\n",
    "print(\"Simulated instances \", simulated_instances)\n",
    "\n",
    "if simulated_instances:\n",
    "    path_baseline_heuristics = \"results/simulated/MFDS_vs_RL/results_test/results_baseline_heuristics.pickle\" \n",
    "else:\n",
    "    path_baseline_heuristics = \"results/real/MFDS_vs_RL/results_versailles_baseline_heuristics.pickle\"    \n",
    "\n",
    "with open(path_baseline_heuristics, \n",
    "          'rb') as handle:\n",
    "    file = pickle.load(handle)\n",
    "    test_names = file[\"res_key_metadata\"]\n",
    "    metric_names = file[\"res_value_metadata\"]\n",
    "    data_res_heur = file[\"data\"]\n",
    "\n",
    "\"\"\"\n",
    "dict_results[(ind_instance, test)] = (flow_val_res, flow_res, m_flow_res, \n",
    "                                        prop_fsupp, prop_sp, trans_func_res)\n",
    "test   -->    [\"min_time\", \n",
    "              \"max_capacity\", \n",
    "              \"trans_func\", \n",
    "              \"random\"]\n",
    "\"\"\"\n",
    "# Unzip to display (each metric on one list)\n",
    "nb_instances = len({id_inst for id_inst, _ in data_res_heur.keys()})\n",
    "dict_perfs_heurs = {test_names[i]:[] for i in range(len(test_names))}\n",
    "for test_name in test_names:\n",
    "    metrics_instances = dict_perfs_heurs[test_name]\n",
    "    for j in range(len(metric_names)):\n",
    "        metrics_instances.append([(id_inst, \n",
    "                                   data_res_heur[(id_inst, test_name)][j]) \n",
    "                                        for id_inst in range(nb_instances)])\n",
    "        metrics_instances[-1].sort(key = lambda x : -x[1])\n",
    "\n",
    "# Display\n",
    "if simulated_instances:\n",
    "    save_path = \"results/simulated/MFDS_vs_RL/results_test/baseline_heuristics/\"\n",
    "else:\n",
    "    save_path = \"results/real/MFDS_vs_RL/baseline_heuristics/\"\n",
    "\n",
    "#save_path = False\n",
    "colors = ['b', 'g', 'k', 'm', 'r', 'y']\n",
    "markers = [\"o\", \"^\", \"s\", \"x\", \"*\", \"h\", \"p\"]\n",
    "show_std = False\n",
    "bins = 'auto'\n",
    "#bins = 100\n",
    "#force = None\n",
    "force = \"bootstrap\"\n",
    "with_pval_norm = False\n",
    "for i in range(len(metric_names)):\n",
    "    fig, fig3 = plt.figure(), plt.figure()\n",
    "    fig2, ax = plt.subplots(len(test_names), figsize=(8, 12))\n",
    "    for j in range(len(test_names)):\n",
    "        test_name = test_names[j]\n",
    "        ### !!!!! size of vals !!!!! bedelha \n",
    "        vals = [e[1] for e in dict_perfs_heurs[test_name][i]]\n",
    "        mean_ = np.mean(vals)\n",
    "        std_ = np.std(vals, ddof=1)\n",
    "        ci, pval_norm = mean_confidence_interval(vals, \n",
    "                                                 confidence_normality_test = 0.95,\n",
    "                                                 force = force,\n",
    "                                                 confidence_level = 0.95)\n",
    "        conf_int_str = \"(\"+str(round(ci[0], 3))+\", \"+str(round(ci[1], 3))+\")\"\n",
    "        added_pval_norm = \" ^ \"+str(round(pval_norm,3)) if with_pval_norm else \"\"\n",
    "        if show_std:\n",
    "            label = test_name+\" \"+str(round(mean_,3))+\"+/-\"+str(round(std_,3))+\" \"+conf_int_str+added_pval_norm\n",
    "        else:\n",
    "            label = test_name+\" \"+str(round(mean_,3))+\" \"+conf_int_str+added_pval_norm\n",
    "        \n",
    "        py.figure(fig.number)\n",
    "        plt.scatter(list(range(len(vals))), \n",
    "                    vals, \n",
    "                    color = colors[j],\n",
    "                    marker = markers[j], \n",
    "                    alpha = 0.5, \n",
    "                    label = label)\n",
    "        \n",
    "        py.figure(fig2.number)\n",
    "        sns.histplot(vals,   \n",
    "                    color = colors[j], \n",
    "                    edgecolor = 'black', \n",
    "                    alpha = 0.5,\n",
    "                    label = label,\n",
    "                    bins = bins,\n",
    "                    kde = True,\n",
    "                    ax = ax[j])\n",
    "        #sns.kdeplot(data = vals, ax = ax[j])\n",
    "        \n",
    "        py.figure(fig3.number)\n",
    "        box = plt.boxplot(vals, \n",
    "                          patch_artist = True,\n",
    "                          widths = 0.6,\n",
    "                          positions = [j+1])\n",
    "        box['boxes'][0].set_facecolor(colors[j])\n",
    "        box['medians'][0].set_color(\"white\")\n",
    "\n",
    "        if metric_names[i] not in {\"prop_flow_support\", \"prop_shortest_paths\"}:\n",
    "            vals_centred = np.array([(v - mean_)/std_ for v in vals])\n",
    "            # Create a qq_plot\n",
    "            fig4 = sm.qqplot(vals_centred, line ='45')\n",
    "            # Save the qqplot\n",
    "            path = os.path.join(save_path,\n",
    "                                \"qq_plots\",\n",
    "                                \"com_base_heurs_\"+metric_names[i]+\"_\"+label+\".png\")\n",
    "            py.figure(fig4.number)\n",
    "            py.savefig(path)\n",
    "            plt.close(fig4)\n",
    "    \n",
    "    py.figure(fig.number)\n",
    "    plt.xlabel(\"Instances\")\n",
    "    plt.ylabel(metric_names[i])\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid(visible = True)\n",
    "    if save_path:\n",
    "        fig.savefig(os.path.join(save_path, \n",
    "                                \"comp_gen_base_heurs_\"+metric_names[i]), \n",
    "                                bbox_inches='tight', \n",
    "                                pad_inches=0)\n",
    "    plt.close(fig)\n",
    "\n",
    "    py.figure(fig2.number)\n",
    "    for k in range(len(test_names)):\n",
    "        #ax[k].set_xlabel(metric_names[i])\n",
    "        #ax[k].set_xlabel(\"Frequency\")\n",
    "        ax[k].legend(loc=\"upper right\")\n",
    "        ax[k].grid(visible = True)\n",
    "    fig2.tight_layout()\n",
    "    if save_path:\n",
    "        fig2.savefig(os.path.join(save_path, \n",
    "                                  \"hist_comp_gen_base_heurs_\"+metric_names[i]), \n",
    "                                  bbox_inches='tight', \n",
    "                                  pad_inches=0)\n",
    "    plt.close(fig2)\n",
    "\n",
    "\n",
    "    py.figure(fig3.number)\n",
    "    plt.xlabel(\"methods\")\n",
    "    plt.ylabel(\"ranges\")\n",
    "    #plt.legend(loc=\"upper right\")\n",
    "    plt.xticks([j+1 for j in range(len(test_names))], test_names)\n",
    "    if save_path:\n",
    "        fig3.savefig(os.path.join(save_path, \n",
    "                                  \"box_comp_gen_base_heurs_\"+metric_names[i]), \n",
    "                                  bbox_inches='tight', \n",
    "                                  pad_inches=0)\n",
    "    plt.close(fig3)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a31e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated instances  False\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "(0, ('rl_node_based', 'one_for_each', (0.33, 0.33, 0.34)))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21252\\455493058.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[0mtest_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"rl_node_based\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"one_for_each\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0.33\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.33\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.34\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetric_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m     metrics_instances_rl_node_based.append([(id_inst, \n\u001b[0m\u001b[0;32m    150\u001b[0m                                             data_res_rl_node_based[(id_inst, test_info)][j])\n\u001b[0;32m    151\u001b[0m                                                 for id_inst in range(nb_instances)])\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21252\\455493058.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetric_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     metrics_instances_rl_node_based.append([(id_inst, \n\u001b[1;32m--> 150\u001b[1;33m                                             data_res_rl_node_based[(id_inst, test_info)][j])\n\u001b[0m\u001b[0;32m    151\u001b[0m                                                 for id_inst in range(nb_instances)])\n\u001b[0;32m    152\u001b[0m     \u001b[0mmetrics_instances_rl_node_based\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: (0, ('rl_node_based', 'one_for_each', (0.33, 0.33, 0.34)))"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "RL Heuristics vs baseline heuristics\n",
    "\"\"\"\n",
    "\n",
    "# Import pickle\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import pylab as py\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\HADDAM\\\\Documents\\\\Python Scripts\\\\multi_flow_decomp\\\\')\n",
    "\n",
    "from RL_methods.utils_stats import mean_confidence_interval, test_twoind_sample\n",
    "\n",
    "\n",
    "\n",
    "##########################  Baseline heuristics handling  ########################## \n",
    "\"\"\"\n",
    "{\"res_key_metadata\":res_key_metadata,\n",
    "\"res_value_metadata\":res_value_metadata, \n",
    "\"data\":deepcopy(dict(dict_results))}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "path_baseline_heuristics = \"results/simulated/MFDS_vs_RL/results_test/results_baseline_heuristics.pickle\"\n",
    "with open(path_baseline_heuristics, \n",
    "          'rb') as handle:\n",
    "    file = pickle.load(handle)\n",
    "    test_names = file[\"res_key_metadata\"]\n",
    "    metric_names = file[\"res_value_metadata\"]\n",
    "    data_res_heur = file[\"data\"]\n",
    "\n",
    "nb_instances = len({id_inst for id_inst, _ in data_res_heur.keys()})\n",
    "\n",
    "\"\"\"\n",
    "dict_results[(ind_instance, test)] = (flow_val_res, flow_res, m_flow_res, \n",
    "                                        prop_fsupp, prop_sp, trans_func_res)\n",
    "test   -->    [\"min_time\", \n",
    "              \"max_capacity\", \n",
    "              \"trans_func\", \n",
    "              \"random\"]\n",
    "\"\"\"\n",
    "# Unzip to display (each metric on one list)\n",
    "metrics_instances_heurs = []\n",
    "for j in range(len(metric_names)):\n",
    "    metrics_instances_heurs.append([(id_inst, \n",
    "                                     data_res_heur[(id_inst, \"trans_func\")][j]) \n",
    "                                        for id_inst in range(nb_instances)])\n",
    "    metrics_instances_heurs[-1].sort(key = lambda x : -x[1])\n",
    "\n",
    "\n",
    "\n",
    "##########################  RL heuristics handling  ##########################\n",
    "\"\"\"\n",
    "{\"res_key_metadata\":res_key_metadata,\n",
    "\"res_value_metadata\":res_value_metadata, \n",
    "\"data\":deepcopy(dict(dict_results))}\n",
    "\"\"\"\n",
    "path_rl_heuristics = \"results/simulated/MFDS_vs_RL/results_test/results_rl_heuristics.pickle\" \n",
    "with open(path_rl_heuristics, \n",
    "          'rb') as handle:\n",
    "    file = pickle.load(handle)\n",
    "    test_infos = file[\"res_key_metadata\"]\n",
    "    #metric_names = file[\"res_value_metadata\"]\n",
    "    data = file[\"data\"]\n",
    "\n",
    "\"\"\"\n",
    "dict_results[(ind_instance, test_infos)] = (flow_val_res, flow_res, \n",
    "                                            m_flow_res, prop_fsupp, \n",
    "                                            prop_sp, trans_func_res,\n",
    "                                            reward)\n",
    "test_infos = (path_type_selector, path_card_criteria, lr_rate, (coeff1, coeff2, coeff3))\n",
    "\"\"\"\n",
    "\n",
    "# The set of all the learning rates\n",
    "lr_rates = set(lr_rate for _, (_, _, lr_rate, _) in data)\n",
    "# The data associated to the node based case\n",
    "data_node_based = {(ind_instance, test_infos):data[(ind_instance, test_infos)]\n",
    "                                                for (ind_instance, test_infos) in data\n",
    "                                                    if test_infos[0] == \"rl_node_based\" and\\\n",
    "                                                        test_infos[1] == \"one_for_each\" and\\\n",
    "                                                            test_infos[3][-1] != 0}\n",
    "# The data associated to the arc based case\n",
    "data_arc_based = {(ind_instance, test_infos):data[(ind_instance, test_infos)]\n",
    "                                                for (ind_instance, test_infos) in data\n",
    "                                                    if test_infos[0] == \"rl_arc_based\" and\\\n",
    "                                                        test_infos[1] == \"one_for_each\" and\\\n",
    "                                                            test_infos[3][-1] != 0}\n",
    "\n",
    "# Calculate the best learning rate for the node_based case\n",
    "mean_rew_by_lr_rate = {np.mean([data_node_based[(ind_instance, test_infos)][-1] \n",
    "                                    for (ind_instance, test_infos) in data_node_based\n",
    "                                        if test_infos[2] == lr_rate]):lr_rate \n",
    "                                            for lr_rate in lr_rates}\n",
    "lr_rate_max_node_based = mean_rew_by_lr_rate[max(mean_rew_by_lr_rate.keys())]\n",
    "\n",
    "# Calculate the best learning rate for the node_based case\n",
    "mean_rew_by_lr_rate = {np.mean([data_arc_based[(ind_instance, test_infos)][-1] \n",
    "                                    for (ind_instance, test_infos) in data_arc_based\n",
    "                                        if test_infos[2] == lr_rate]):lr_rate \n",
    "                                            for lr_rate in lr_rates}\n",
    "lr_rate_max_arc_based = mean_rew_by_lr_rate[max(mean_rew_by_lr_rate.keys())]\n",
    "\n",
    "\n",
    "# Filter to leave only the best value of the learning parameter for the node based case\n",
    "data_res_rl_node_based = {(ind_instance, \n",
    "                            (test_infos[0], \n",
    "                            test_infos[1], \n",
    "                            test_infos[3])):data_node_based[(ind_instance, test_infos)]+(lr_rate_max_node_based,)\n",
    "                                                for (ind_instance, test_infos) in data_node_based\n",
    "                                                    if test_infos[2] == lr_rate_max_node_based}\n",
    "\n",
    "\n",
    "# Filter to leave only the best value of the learning parameter for the arc based case\n",
    "data_res_rl_arc_based = {(ind_instance, \n",
    "                            (test_infos[0], \n",
    "                            test_infos[1], \n",
    "                            test_infos[3])):data_arc_based[(ind_instance, test_infos)]+(lr_rate_max_arc_based,)\n",
    "                                                for (ind_instance, test_infos) in data_arc_based\n",
    "                                                    if test_infos[2] == lr_rate_max_arc_based}\n",
    "\n",
    "# Unzip to display (each metric on one list)\n",
    "metrics_instances_rl_node_based = []\n",
    "test_info = (\"rl_node_based\", \"one_for_each\", (0.33, 0.33, 0.34))\n",
    "for j in range(len(metric_names)):\n",
    "    metrics_instances_rl_node_based.append([(id_inst, \n",
    "                                            data_res_rl_node_based[(id_inst, test_info)][j])\n",
    "                                                for id_inst in range(nb_instances)])\n",
    "    metrics_instances_rl_node_based[-1].sort(key = lambda x : -x[1])\n",
    "\n",
    "metrics_instances_rl_arc_based = []\n",
    "test_info = (\"rl_arc_based\", \"one_for_each\", (0.33, 0.33, 0.34))\n",
    "for j in range(len(metric_names)):\n",
    "    metrics_instances_rl_arc_based.append([(id_inst, \n",
    "                                            data_res_rl_arc_based[(id_inst, test_info)][j])\n",
    "                                                for id_inst in range(nb_instances)])\n",
    "    metrics_instances_rl_arc_based[-1].sort(key = lambda x : -x[1])\n",
    "\n",
    "# Diplay the curves\n",
    "save_path = \"results/simulated/MFDS_vs_RL/results_test/rl_vs_baseline\"\n",
    "colors = ['b', 'g', 'k', 'r', 'y', 'brown', 'purple', 'orange',  'gray', 'pink']\n",
    "markers = [\"o\", \"^\", \"s\", \"x\", \"*\", \"h\", \"p\", \"+\"]\n",
    "bins = 'auto'\n",
    "#bins = 100\n",
    "#force = None\n",
    "force = \"bootstrap\"\n",
    "with_pval_norm = False\n",
    "for i in range(len(metric_names)):\n",
    "    fig, fig3 = plt.figure(), plt.figure()\n",
    "    fig2, ax = plt.subplots(3, figsize=(8, 12))\n",
    "    \n",
    "    # Construct list of values of the metric for each instance\n",
    "    vals = [e[1] for e in metrics_instances_heurs[i]]\n",
    "    # Process mean and reward\n",
    "    mean_ = np.mean(vals)\n",
    "    std_ = np.std(vals, ddof=1)\n",
    "    ci, pval_norm = mean_confidence_interval(vals, \n",
    "                                             confidence_normality_test = 0.95,\n",
    "                                             force = force,\n",
    "                                             confidence_level = 0.95)\n",
    "    name = \"trans_func\"\n",
    "    conf_int_str = \"(\"+str(round(ci[0], 3))+\", \"+str(round(ci[1], 3))+\")\"\n",
    "    added_pval_norm = \" ^ \"+str(round(pval_norm,3)) if with_pval_norm else \"\"\n",
    "    label = name+\" \"+str(round(mean_,3))+\" \"+conf_int_str+added_pval_norm\n",
    "    # Plot figure 1\n",
    "    py.figure(fig.number)\n",
    "    plt.scatter(list(range(len(vals))), \n",
    "                vals, \n",
    "                color = colors[0],\n",
    "                marker = markers[0], \n",
    "                alpha = 0.5, \n",
    "                label = label)\n",
    "    # Plot figure 2\n",
    "    py.figure(fig2.number)\n",
    "    sns.histplot(vals,\n",
    "                 color = colors[0], \n",
    "                 edgecolor = 'black', \n",
    "                 alpha = 0.5,\n",
    "                 bins = bins,\n",
    "                 kde = True,\n",
    "                 label = label,\n",
    "                 ax = ax[0])\n",
    "    \n",
    "    # Plot figure 3\n",
    "    py.figure(fig3.number)\n",
    "    box = plt.boxplot(vals, \n",
    "                      patch_artist = True,\n",
    "                      widths = 0.6,\n",
    "                      positions = [1])\n",
    "    box['boxes'][0].set_facecolor(colors[0])\n",
    "    box['medians'][0].set_color(\"white\")\n",
    "    # Plot qq\n",
    "    if metric_names[i] not in {\"prop_flow_support\", \"prop_shortest_paths\"}:\n",
    "        vals_centred = np.array([(v - mean_)/std_ for v in vals])\n",
    "        # Create a qq_plot\n",
    "        fig4 = sm.qqplot(vals_centred, line ='45')\n",
    "        # Save the qqplot\n",
    "        path = os.path.join(save_path,\n",
    "                            \"qq_plots\",\n",
    "                            \"comp_rl_vs_baseline_\"+metric_names[i]+\"_\"+label+\".png\")\n",
    "        py.figure(fig4.number)\n",
    "        py.savefig(path)\n",
    "        plt.close(fig4)\n",
    "    \n",
    "\n",
    "    # Construct list of values of the metric for each instance\n",
    "    vals = [e[1] for e in metrics_instances_rl_node_based[i]]\n",
    "    # Process mean and reward\n",
    "    mean_ = np.mean(vals)\n",
    "    std_ = np.std(vals, ddof=1)\n",
    "    ci, pval_norm = mean_confidence_interval(vals, \n",
    "                                             confidence_normality_test = 0.95,\n",
    "                                             force = force,\n",
    "                                             confidence_level = 0.95)\n",
    "    name = \"rl_node_based\"\n",
    "    conf_int_str = \"(\"+str(round(ci[0], 3))+\", \"+str(round(ci[1], 3))+\")\"\n",
    "    added_pval_norm = \" ^ \"+str(round(pval_norm,3)) if with_pval_norm else \"\"\n",
    "    label = name+\" \"+str(round(mean_,3))+\" \"+conf_int_str+added_pval_norm\n",
    "    # Plot figure 1\n",
    "    py.figure(fig.number)\n",
    "    plt.scatter(list(range(len(vals))), \n",
    "                vals, \n",
    "                color = colors[1],\n",
    "                marker = markers[1], \n",
    "                alpha = 0.5, \n",
    "                label = label)\n",
    "    # Plot figure 2\n",
    "    py.figure(fig2.number)\n",
    "    sns.histplot(vals,\n",
    "                 color = colors[1], \n",
    "                 edgecolor = 'black', \n",
    "                 alpha = 0.5,\n",
    "                 bins = bins,\n",
    "                 kde = True,\n",
    "                 label = label,\n",
    "                 ax = ax[1])\n",
    "    \n",
    "    # Plot figure 3\n",
    "    py.figure(fig3.number)\n",
    "    box = plt.boxplot(vals, \n",
    "                      patch_artist = True,\n",
    "                      widths = 0.6,\n",
    "                      positions = [2])\n",
    "    box['boxes'][0].set_facecolor(colors[1])\n",
    "    box['medians'][0].set_color(\"white\")\n",
    "    # Plot qq\n",
    "    if metric_names[i] not in {\"prop_flow_support\", \"prop_shortest_paths\"}:\n",
    "        vals_centred = np.array([(v - mean_)/std_ for v in vals])\n",
    "        # Create a qq_plot\n",
    "        fig4 = sm.qqplot(vals_centred, line ='45')\n",
    "        # Save the qqplot\n",
    "        path = os.path.join(save_path,\n",
    "                            \"qq_plots\",\n",
    "                            \"comp_rl_vs_baseline_\"+metric_names[i]+\"_\"+label+\".png\")\n",
    "        py.figure(fig4.number)\n",
    "        py.savefig(path)\n",
    "        plt.close(fig4)\n",
    "    \n",
    "    \n",
    "    # Construct list of values of the metric for each instance\n",
    "    vals = [e[1] for e in metrics_instances_rl_arc_based[i]]\n",
    "    # Process mean and reward\n",
    "    mean_ = np.mean(vals)\n",
    "    std_ = np.std(vals, ddof=1)\n",
    "    ci, pval_norm = mean_confidence_interval(vals, \n",
    "                                             confidence_normality_test = 0.95,\n",
    "                                             force = force,\n",
    "                                             confidence_level = 0.95)\n",
    "    name = \"rl_arc_based\"\n",
    "    conf_int_str = \"(\"+str(round(ci[0], 3))+\", \"+str(round(ci[1], 3))+\")\"\n",
    "    added_pval_norm = \" ^ \"+str(round(pval_norm,3)) if with_pval_norm else \"\"\n",
    "    label = name+\" \"+str(round(mean_,3))+\" \"+conf_int_str+added_pval_norm\n",
    "    # Plot figure 1\n",
    "    py.figure(fig.number)\n",
    "    plt.scatter(list(range(len(vals))), \n",
    "                vals, \n",
    "                color = colors[2],\n",
    "                marker = markers[2], \n",
    "                alpha = 0.5, \n",
    "                label = label)\n",
    "    # Plot figure 2\n",
    "    py.figure(fig2.number)\n",
    "    sns.histplot(vals,\n",
    "                 color = colors[2], \n",
    "                 edgecolor = 'black', \n",
    "                 alpha = 0.5,\n",
    "                 bins = bins,\n",
    "                 kde = True,\n",
    "                 label = label,\n",
    "                 ax = ax[2])\n",
    "    \n",
    "    # Plot figure 3\n",
    "    py.figure(fig3.number)\n",
    "    box = plt.boxplot(vals, \n",
    "                      patch_artist = True,\n",
    "                      widths = 0.6,\n",
    "                      positions = [3])\n",
    "    box['boxes'][0].set_facecolor(colors[2])\n",
    "    box['medians'][0].set_color(\"white\")\n",
    "    # Plot qq\n",
    "    if metric_names[i] not in {\"prop_flow_support\", \"prop_shortest_paths\"}:\n",
    "        vals_centred = np.array([(v - mean_)/std_ for v in vals])\n",
    "        # Create a qq_plot\n",
    "        fig4 = sm.qqplot(vals_centred, line ='45')\n",
    "        # Save the qqplot\n",
    "        path = os.path.join(save_path,\n",
    "                            \"qq_plots\",\n",
    "                            \"comp_rl_vs_baseline_\"+metric_names[i]+\"_\"+label+\".png\")\n",
    "        py.figure(fig4.number)\n",
    "        py.savefig(path)\n",
    "        plt.close(fig4)\n",
    "    \n",
    "    py.figure(fig.number)\n",
    "    plt.xlabel(\"Instances\")\n",
    "    plt.ylabel(metric_names[i])\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid(visible = True)\n",
    "    if save_path:\n",
    "        fig.savefig(os.path.join(save_path, \n",
    "                                \"comp_rl_vs_baseline_\"+metric_names[i]), \n",
    "                                bbox_inches='tight', \n",
    "                                pad_inches=0)\n",
    "    plt.close(fig)\n",
    "\n",
    "    py.figure(fig2.number)\n",
    "    for k in range(3):\n",
    "        #ax[k].set_xlabel(metric_names[i])\n",
    "        #ax[k].set_xlabel(\"Frequency\")\n",
    "        ax[k].legend(loc=\"upper right\")\n",
    "        ax[k].grid(visible = True)\n",
    "    fig2.tight_layout()\n",
    "    if save_path:\n",
    "        fig2.savefig(os.path.join(save_path, \n",
    "                                  \"hist_comp_rl_vs_baseline_\"+metric_names[i]), \n",
    "                                  bbox_inches='tight', \n",
    "                                  pad_inches=0)\n",
    "    plt.close(fig2)\n",
    "\n",
    "\n",
    "    py.figure(fig3.number)\n",
    "    plt.xlabel(\"methods\")\n",
    "    plt.ylabel(\"ranges\")\n",
    "    #plt.legend(loc=\"upper right\")\n",
    "    plt.xticks([j+1 for j in range(3)], [\"trans_func\", \"rl_node_based\", \"rl_arc_based\"])\n",
    "    if save_path:\n",
    "        fig3.savefig(os.path.join(save_path, \n",
    "                                  \"box_comp_rl_vs_baseline_\"+metric_names[i]), \n",
    "                                  bbox_inches='tight', \n",
    "                                  pad_inches=0)\n",
    "    plt.close(fig3)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "\n",
    "for i in range(len(metric_names)):\n",
    "    if metric_names[i] not in {\"prop_flow_support\", \"prop_shortest_paths\"}:\n",
    "        # Compare multi flow residue with satistics test\n",
    "        vals_mf_node_based = [e[1] for e in metrics_instances_rl_node_based[i]]\n",
    "\n",
    "\n",
    "        # Construct list of values of the metric for each instance\n",
    "        vals_mf_arc_based = [e[1] for e in metrics_instances_rl_arc_based[i]]\n",
    "\n",
    "\n",
    "        dict_result_ge = test_twoind_sample(vals_mf_node_based, \n",
    "                                        vals_mf_arc_based,\n",
    "                                        confidence_normality_test = 0.95,\n",
    "                                        confidence_level = 0.95,\n",
    "                                        force = force,\n",
    "                                        alternative_hypothesis = \"less\")\n",
    "\n",
    "\n",
    "        dict_result_eq = test_twoind_sample(vals_mf_node_based, \n",
    "                                        vals_mf_arc_based,\n",
    "                                        confidence_normality_test = 0.95,\n",
    "                                        confidence_level = 0.95,\n",
    "                                        force = force,\n",
    "                                        alternative_hypothesis = \"two-sided\")\n",
    "\n",
    "\n",
    "        dict_result_le = test_twoind_sample(vals_mf_node_based, \n",
    "                                        vals_mf_arc_based,\n",
    "                                        confidence_normality_test = 0.95,\n",
    "                                        confidence_level = 0.95,\n",
    "                                        force = force,\n",
    "                                        alternative_hypothesis = \"greater\")\n",
    "\n",
    "\n",
    "        # Equality test\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "        print(\"Result of hypothesis testing vals_mf_node_based - vals_mf_arc_based for \"+metric_names[i])\n",
    "        print(\"H0:ge \", dict_result_ge)\n",
    "        print(\"H0:eq \", dict_result_eq)\n",
    "        print(\"H0:le \", dict_result_le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2d53c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "Result of hypothesis testing vals_mf_1path - vals_mf_mulpath for flow_val_residue\n",
      "H0:ge  {'decision': 'not reject', 'pvalue': 0.99312}\n",
      "H0:eq  {'decision': 'reject', 'pvalue': 0.01374}\n",
      "H0:le  {'decision': 'reject', 'pvalue': 0.00621}\n",
      "-----------------------------------------------------------------------------\n",
      "Result of hypothesis testing vals_mf_1path - vals_mf_mulpath for flow_residue\n",
      "H0:ge  {'decision': 'not reject', 'pvalue': 0.97283}\n",
      "H0:eq  {'decision': 'not reject', 'pvalue': 0.05512}\n",
      "H0:le  {'decision': 'reject', 'pvalue': 0.02658}\n",
      "-----------------------------------------------------------------------------\n",
      "Result of hypothesis testing vals_mf_1path - vals_mf_mulpath for multi_flow_residue\n",
      "H0:ge  {'decision': 'not reject', 'pvalue': 0.89718}\n",
      "H0:eq  {'decision': 'not reject', 'pvalue': 0.20486}\n",
      "H0:le  {'decision': 'not reject', 'pvalue': 0.10085}\n",
      "-----------------------------------------------------------------------------\n",
      "Result of hypothesis testing vals_mf_1path - vals_mf_mulpath for transition_function_residue\n",
      "H0:ge  {'decision': 'not reject', 'pvalue': 0.92136}\n",
      "H0:eq  {'decision': 'not reject', 'pvalue': 0.15713}\n",
      "H0:le  {'decision': 'not reject', 'pvalue': 0.0788}\n",
      "-----------------------------------------------------------------------------\n",
      "Result of hypothesis testing vals_mf_1path - vals_mf_mulpath for reward\n",
      "H0:ge  {'decision': 'reject', 'pvalue': 0.02298}\n",
      "H0:eq  {'decision': 'reject', 'pvalue': 0.04356}\n",
      "H0:le  {'decision': 'not reject', 'pvalue': 0.97758}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "RL Heuristics one path vs multiple paths\n",
    "\"\"\"\n",
    "\n",
    "# Import pickle\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import pylab as py\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\HADDAM\\\\Documents\\\\Python Scripts\\\\multi_flow_decomp\\\\')\n",
    "\n",
    "from RL_methods.utils_stats import mean_confidence_interval, test_twoind_sample\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "{\"res_key_metadata\":res_key_metadata,\n",
    "\"res_value_metadata\":res_value_metadata, \n",
    "\"data\":deepcopy(dict(dict_results))}\n",
    "\"\"\"\n",
    "with open(\"results/simulated/MFDS_vs_RL/results_test/results_rl_heuristics.pickle\", \n",
    "          'rb') as handle:\n",
    "    file = pickle.load(handle)\n",
    "    test_infos = file[\"res_key_metadata\"]\n",
    "    metric_names = file[\"res_value_metadata\"]\n",
    "    data = file[\"data\"]\n",
    "\n",
    "\"\"\"\n",
    "dict_results[(ind_instance, test_infos)] = (flow_val_res, flow_res, \n",
    "                                            m_flow_res, prop_fsupp, \n",
    "                                            prop_sp, trans_func_res,\n",
    "                                            reward)\n",
    "test_infos = (path_type_selector, path_card_criteria, lr_rate, (coeff1, coeff2, coeff3))\n",
    "\"\"\"\n",
    "\n",
    "# The set of all the learning rates\n",
    "lr_rates = set(lr_rate for _, (_, _, lr_rate, _) in data)\n",
    "# The number of instances\n",
    "nb_instances = len({id_inst for id_inst, _ in data.keys()})\n",
    "# The data associated to the one path case\n",
    "data_one_only = {(ind_instance, test_infos):data[(ind_instance, test_infos)]\n",
    "                                                for (ind_instance, test_infos) in data\n",
    "                                                    if test_infos[0] == \"rl_arc_based\" and\\\n",
    "                                                        test_infos[1] == \"one_only\" and\\\n",
    "                                                            test_infos[3][-1] != 0}\n",
    "# The data associated to the multiple path case\n",
    "data_one_for_each = {(ind_instance, test_infos):data[(ind_instance, test_infos)]\n",
    "                                                    for (ind_instance, test_infos) in data\n",
    "                                                        if test_infos[0] == \"rl_arc_based\" and\\\n",
    "                                                            test_infos[1] == \"one_for_each\" and\\\n",
    "                                                                test_infos[3][-1] != 0}\n",
    "\n",
    "# Calculate the best learning rate for the one path case\n",
    "mean_rew_by_lr_rate = {np.mean([data_one_only[(ind_instance, test_infos)][-1] \n",
    "                                    for (ind_instance, test_infos) in data_one_only\n",
    "                                        if test_infos[2] == lr_rate]):lr_rate \n",
    "                                            for lr_rate in lr_rates}\n",
    "lr_rate_max_one_only = mean_rew_by_lr_rate[max(mean_rew_by_lr_rate.keys())]\n",
    "\n",
    "# Calculate the best learning rate for the multiple path case\n",
    "mean_rew_by_lr_rate = {np.mean([data_one_for_each[(ind_instance, test_infos)][-1] \n",
    "                                    for (ind_instance, test_infos) in data_one_for_each\n",
    "                                        if test_infos[2] == lr_rate]):lr_rate \n",
    "                                            for lr_rate in lr_rates}\n",
    "lr_rate_max_one_for_each = mean_rew_by_lr_rate[max(mean_rew_by_lr_rate.keys())]\n",
    "\n",
    "\n",
    "# Filter to leave only the best value of the learning parameter for the one path case\n",
    "data_res_rl_one_only = {(ind_instance, \n",
    "                        (test_infos[0], \n",
    "                        test_infos[1], \n",
    "                        test_infos[3])):data_one_only[(ind_instance, test_infos)]+(lr_rate_max_one_only,)\n",
    "                                        for (ind_instance, test_infos) in data_one_only\n",
    "                                            if test_infos[2] == lr_rate_max_one_only}\n",
    "\n",
    "\n",
    "# Filter to leave only the best value of the learning parameter for the multiple path case\n",
    "data_res_rl_one_for_each = {(ind_instance, \n",
    "                            (test_infos[0], \n",
    "                            test_infos[1], \n",
    "                            test_infos[3])):data_one_for_each[(ind_instance, test_infos)]+(lr_rate_max_one_for_each,)\n",
    "                                                for (ind_instance, test_infos) in data_one_for_each\n",
    "                                                    if test_infos[2] == lr_rate_max_one_for_each}\n",
    "\n",
    "# Unzip to display (each metric on one list)\n",
    "metrics_instances_rl_one_path = []\n",
    "test_info = (\"rl_arc_based\", \"one_only\", (0.33, 0.33, 0.34))\n",
    "for j in range(len(metric_names)):\n",
    "    metrics_instances_rl_one_path.append([(id_inst, \n",
    "                                            data_res_rl_one_only[(id_inst, test_info)][j])\n",
    "                                                for id_inst in range(nb_instances)])\n",
    "    metrics_instances_rl_one_path[-1].sort(key = lambda x : -x[1])\n",
    "\n",
    "metrics_instances_rl_one_for_each = []\n",
    "test_info = (\"rl_arc_based\", \"one_for_each\", (0.33, 0.33, 0.34))\n",
    "for j in range(len(metric_names)):\n",
    "    metrics_instances_rl_one_for_each.append([(id_inst, \n",
    "                                                data_res_rl_one_for_each[(id_inst, test_info)][j])\n",
    "                                                    for id_inst in range(nb_instances)])\n",
    "    metrics_instances_rl_one_for_each[-1].sort(key = lambda x : -x[1])\n",
    "\n",
    "\n",
    "# Diplay the curves\n",
    "save_path = \"results/simulated/MFDS_vs_RL/results_test/one_path_vs_multiple_paths\"\n",
    "colors = ['b', 'g', 'k', 'r', 'y', 'brown', 'purple', 'orange',  'gray', 'pink']\n",
    "markers = [\"o\", \"^\", \"s\", \"x\", \"*\", \"h\", \"p\", \"+\"]\n",
    "bins = 'auto'\n",
    "#bins = 100\n",
    "#force = None\n",
    "force = \"bootstrap\"\n",
    "with_pval_norm = False\n",
    "for i in range(len(metric_names)):\n",
    "    fig, fig3 = plt.figure(), plt.figure()\n",
    "    fig2, ax = plt.subplots(2, figsize=(8, 12))\n",
    "\n",
    "    # Construct list of values of the metric for each instance\n",
    "    vals = [e[1] for e in metrics_instances_rl_one_path[i]]\n",
    "    # Process mean and reward\n",
    "    mean_ = np.mean(vals)\n",
    "    std_ = np.std(vals, ddof=1)\n",
    "    ci, pval_norm = mean_confidence_interval(vals, \n",
    "                                             confidence_normality_test = 0.95,\n",
    "                                             force = force,\n",
    "                                             confidence_level = 0.95)\n",
    "    name = \"rl_arc_one_path_based\"\n",
    "    conf_int_str = \"(\"+str(round(ci[0], 3))+\", \"+str(round(ci[1], 3))+\")\"\n",
    "    added_pval_norm = \" ^ \"+str(round(pval_norm,3)) if with_pval_norm else \"\"\n",
    "    label = name+\" \"+str(round(mean_,3))+\" \"+conf_int_str+added_pval_norm\n",
    "    # Plot figure 1\n",
    "    py.figure(fig.number)\n",
    "    plt.scatter(list(range(len(vals))), \n",
    "                vals, \n",
    "                color = colors[0],\n",
    "                marker = markers[0], \n",
    "                alpha = 0.5, \n",
    "                label = label)\n",
    "    # Plot figure 2\n",
    "    py.figure(fig2.number)\n",
    "    sns.histplot(vals,\n",
    "                 color = colors[0], \n",
    "                 edgecolor = 'black', \n",
    "                 alpha = 0.5,\n",
    "                 bins = bins,\n",
    "                 kde = True,\n",
    "                 label = label,\n",
    "                 ax = ax[0])\n",
    "    \n",
    "    # Plot figure 3\n",
    "    py.figure(fig3.number)\n",
    "    box = plt.boxplot(vals, \n",
    "                      patch_artist = True,\n",
    "                      widths = 0.6,\n",
    "                      positions = [1])\n",
    "    box['boxes'][0].set_facecolor(colors[0])\n",
    "    box['medians'][0].set_color(\"white\")\n",
    "    # Plot qq\n",
    "    if metric_names[i] not in {\"prop_flow_support\", \"prop_shortest_paths\"}:\n",
    "        vals_centred = np.array([(v - mean_)/std_ for v in vals])\n",
    "        # Create a qq_plot\n",
    "        fig4 = sm.qqplot(vals_centred, line ='45')\n",
    "        # Save the qqplot\n",
    "        path = os.path.join(save_path,\n",
    "                            \"qq_plots\",\n",
    "                            \"comp_rl_one_vs_multi_\"+metric_names[i]+\"_\"+label+\".png\")\n",
    "        py.figure(fig4.number)\n",
    "        py.savefig(path)\n",
    "        plt.close(fig4)\n",
    "    \n",
    "    # Construct list of values of the metric for each instance\n",
    "    vals = [e[1] for e in metrics_instances_rl_one_for_each[i]]\n",
    "    # Process mean and reward\n",
    "    mean_ = np.mean(vals)\n",
    "    std_ = np.std(vals, ddof=1)\n",
    "    ci, pval_norm = mean_confidence_interval(vals, \n",
    "                                             confidence_normality_test = 0.95,\n",
    "                                             force = force,\n",
    "                                             confidence_level = 0.95)\n",
    "    name = \"rl_arc_multi_path_based\"\n",
    "    conf_int_str = \"(\"+str(round(ci[0], 3))+\", \"+str(round(ci[1], 3))+\")\"\n",
    "    added_pval_norm = \" ^ \"+str(round(pval_norm,3)) if with_pval_norm else \"\"\n",
    "    label = name+\" \"+str(round(mean_,3))+\" \"+conf_int_str+added_pval_norm\n",
    "    # Plot figure 1\n",
    "    py.figure(fig.number)\n",
    "    plt.scatter(list(range(len(vals))), \n",
    "                vals, \n",
    "                color = colors[1],\n",
    "                marker = markers[1], \n",
    "                alpha = 0.5, \n",
    "                label = label)\n",
    "    # Plot figure 2\n",
    "    py.figure(fig2.number)\n",
    "    sns.histplot(vals,\n",
    "                 color = colors[1], \n",
    "                 edgecolor = 'black', \n",
    "                 alpha = 0.5,\n",
    "                 bins = bins,\n",
    "                 kde = True,\n",
    "                 label = label,\n",
    "                 ax = ax[1])\n",
    "    \n",
    "    # Plot figure 3\n",
    "    py.figure(fig3.number)\n",
    "    box = plt.boxplot(vals, \n",
    "                      patch_artist = True,\n",
    "                      widths = 0.6,\n",
    "                      positions = [2])\n",
    "    box['boxes'][0].set_facecolor(colors[1])\n",
    "    box['medians'][0].set_color(\"white\")\n",
    "    # Plot qq\n",
    "    if metric_names[i] not in {\"prop_flow_support\", \"prop_shortest_paths\"}:\n",
    "        vals_centred = np.array([(v - mean_)/std_ for v in vals])\n",
    "        # Create a qq_plot\n",
    "        fig4 = sm.qqplot(vals_centred, line ='45')\n",
    "        # Save the qqplot\n",
    "        path = os.path.join(save_path,\n",
    "                            \"qq_plots\",\n",
    "                            \"comp_rl_one_vs_multi_\"+metric_names[i]+\"_\"+label+\".png\")\n",
    "        py.figure(fig4.number)\n",
    "        py.savefig(path)\n",
    "        plt.close(fig4)\n",
    "\n",
    "\n",
    "    py.figure(fig.number)\n",
    "    plt.xlabel(\"Instances\")\n",
    "    plt.ylabel(metric_names[i])\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid(visible = True)\n",
    "    if save_path:\n",
    "        fig.savefig(os.path.join(save_path, \n",
    "                                \"comp_rl_one_vs_multi_\"+metric_names[i]), \n",
    "                                bbox_inches='tight', \n",
    "                                pad_inches=0)\n",
    "    plt.close(fig)\n",
    "\n",
    "    py.figure(fig2.number)\n",
    "    for k in range(2):\n",
    "        #ax[k].set_xlabel(metric_names[i])\n",
    "        #ax[k].set_xlabel(\"Frequency\")\n",
    "        ax[k].legend(loc=\"upper right\")\n",
    "        ax[k].grid(visible = True)\n",
    "    fig2.tight_layout()\n",
    "    if save_path:\n",
    "        fig2.savefig(os.path.join(save_path, \n",
    "                                  \"hist_comp_rl_one_vs_multi_\"+metric_names[i]), \n",
    "                                  bbox_inches='tight', \n",
    "                                  pad_inches=0)\n",
    "    plt.close(fig2)\n",
    "\n",
    "\n",
    "    py.figure(fig3.number)\n",
    "    plt.xlabel(\"methods\")\n",
    "    plt.ylabel(\"ranges\")\n",
    "    #plt.legend(loc=\"upper right\")\n",
    "    plt.xticks([j+1 for j in range(2)], [\"rl_arc_one_path_based\", \"rl_arc_multi_path_based\"])\n",
    "    if save_path:\n",
    "        fig3.savefig(os.path.join(save_path, \n",
    "                                  \"box_comp_rl_one_vs_multi_\"+metric_names[i]), \n",
    "                                  bbox_inches='tight', \n",
    "                                  pad_inches=0)\n",
    "    plt.close(fig3)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "\n",
    "for i in range(len(metric_names)):\n",
    "    if metric_names[i] not in {\"prop_flow_support\", \"prop_shortest_paths\"}:\n",
    "        # Compare multi flow residue with satistics test\n",
    "        vals_mf_one_path = [e[1] for e in metrics_instances_rl_one_path[i]]\n",
    "\n",
    "        # Construct list of values of the metric for each instance\n",
    "        vals_mf_one_for_each = [e[1] for e in metrics_instances_rl_one_for_each[i]]\n",
    "\n",
    "\n",
    "        dict_result_ge = test_twoind_sample(vals_mf_one_path, \n",
    "                                        vals_mf_one_for_each,\n",
    "                                        confidence_normality_test = 0.95,\n",
    "                                        confidence_level = 0.95,\n",
    "                                        force = force,\n",
    "                                        alternative_hypothesis = \"less\")\n",
    "\n",
    "        dict_result_eq = test_twoind_sample(vals_mf_one_path, \n",
    "                                        vals_mf_one_for_each,\n",
    "                                        confidence_normality_test = 0.95,\n",
    "                                        confidence_level = 0.95,\n",
    "                                        force = force,\n",
    "                                        alternative_hypothesis = \"two-sided\")\n",
    "\n",
    "\n",
    "        dict_result_le = test_twoind_sample(vals_mf_one_path, \n",
    "                                        vals_mf_one_for_each,\n",
    "                                        confidence_normality_test = 0.95,\n",
    "                                        confidence_level = 0.95,\n",
    "                                        force = force,\n",
    "                                        alternative_hypothesis = \"greater\")\n",
    "\n",
    "\n",
    "        # Equality test\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "        print(\"Result of hypothesis testing vals_mf_1path - vals_mf_mulpath for \"+metric_names[i])\n",
    "        print(\"H0:ge \", dict_result_ge)\n",
    "        print(\"H0:eq \", dict_result_eq)\n",
    "        print(\"H0:le \", dict_result_le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c29d00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RL Heuristics trans function reward vs no trans function reward\n",
    "\"\"\"\n",
    "\n",
    "# Import pickle\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import pylab as py\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\HADDAM\\\\Documents\\\\Python Scripts\\\\multi_flow_decomp\\\\')\n",
    "\n",
    "from RL_methods.utils_stats import mean_confidence_interval\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "{\"res_key_metadata\":res_key_metadata,\n",
    "\"res_value_metadata\":res_value_metadata, \n",
    "\"data\":deepcopy(dict(dict_results))}\n",
    "\"\"\"\n",
    "with open(\"results/simulated/MFDS_vs_RL/results_test/results_rl_heuristics.pickle\", \n",
    "          'rb') as handle:\n",
    "    file = pickle.load(handle)\n",
    "    test_infos = file[\"res_key_metadata\"]\n",
    "    metric_names = file[\"res_value_metadata\"]\n",
    "    data = file[\"data\"]\n",
    "\n",
    "\"\"\"\n",
    "dict_results[(ind_instance, test_infos)] = (flow_val_res, flow_res, \n",
    "                                            m_flow_res, prop_fsupp, \n",
    "                                            prop_sp, trans_func_res,\n",
    "                                            reward)\n",
    "test_infos = (path_type_selector, path_card_criteria, lr_rate, (coeff1, coeff2, coeff3))\n",
    "\"\"\"\n",
    "\n",
    "# The set of all the learning rates\n",
    "lr_rates = set(lr_rate for _, (_, _, lr_rate, _) in data)\n",
    "# The number of instances\n",
    "nb_instances = len({id_inst for id_inst, _ in data.keys()})\n",
    "# The data associated to the one path case\n",
    "data_no_trans_rew = {(ind_instance, test_infos):data[(ind_instance, test_infos)]\n",
    "                                                    for (ind_instance, test_infos) in data\n",
    "                                                        if test_infos[0] == \"rl_arc_based\" and\\\n",
    "                                                            test_infos[1] == \"one_for_each\" and\\\n",
    "                                                                test_infos[3][-1] == 0}\n",
    "\n",
    "# The data associated to the multiple path case\n",
    "data_trans_rew = {(ind_instance, test_infos):data[(ind_instance, test_infos)]\n",
    "                                                    for (ind_instance, test_infos) in data\n",
    "                                                        if test_infos[0] == \"rl_arc_based\" and\\\n",
    "                                                            test_infos[1] == \"one_for_each\" and\\\n",
    "                                                                test_infos[3][-1] != 0}\n",
    "\n",
    "# Calculate the best learning rate for the one path case\n",
    "mean_rew_by_lr_rate = {np.mean([data_no_trans_rew[(ind_instance, test_infos)][-1] \n",
    "                                    for (ind_instance, test_infos) in data_no_trans_rew\n",
    "                                        if test_infos[2] == lr_rate]):lr_rate \n",
    "                                            for lr_rate in lr_rates}\n",
    "lr_rate_max_no_trans = mean_rew_by_lr_rate[max(mean_rew_by_lr_rate.keys())]\n",
    "\n",
    "# Calculate the best learning rate for the multiple path case\n",
    "mean_rew_by_lr_rate = {np.mean([data_trans_rew[(ind_instance, test_infos)][-1] \n",
    "                                    for (ind_instance, test_infos) in data_trans_rew\n",
    "                                        if test_infos[2] == lr_rate]):lr_rate \n",
    "                                            for lr_rate in lr_rates}\n",
    "lr_rate_max_trans = mean_rew_by_lr_rate[max(mean_rew_by_lr_rate.keys())]\n",
    "\n",
    "\n",
    "# Filter to leave only the best value of the learning parameter for the one path case\n",
    "data_res_no_trans_rew = {(ind_instance, \n",
    "                        (test_infos[0], \n",
    "                        test_infos[1], \n",
    "                        test_infos[3])):data_no_trans_rew[(ind_instance, test_infos)]+(lr_rate_max_no_trans,)\n",
    "                                        for (ind_instance, test_infos) in data_no_trans_rew\n",
    "                                            if test_infos[2] == lr_rate_max_no_trans}\n",
    "\n",
    "\n",
    "# Filter to leave only the best value of the learning parameter for the multiple path case\n",
    "data_res_trans_rew = {(ind_instance, \n",
    "                       (test_infos[0], \n",
    "                        test_infos[1], \n",
    "                        test_infos[3])):data_trans_rew[(ind_instance, test_infos)]+(lr_rate_max_trans,)\n",
    "                                            for (ind_instance, test_infos) in data_trans_rew\n",
    "                                                if test_infos[2] == lr_rate_max_trans}\n",
    "\n",
    "# Unzip to display (each metric on one list)\n",
    "metrics_instances_no_trans = []\n",
    "test_info = (\"rl_arc_based\", \"one_for_each\", (0.5, 0.5, 0.0))\n",
    "for j in range(len(metric_names)):\n",
    "    metrics_instances_no_trans.append([(id_inst, \n",
    "                                        data_res_no_trans_rew[(id_inst, test_info)][j])\n",
    "                                            for id_inst in range(nb_instances)])\n",
    "    metrics_instances_no_trans[-1].sort(key = lambda x : -x[1])\n",
    "\n",
    "metrics_instances_trans = []\n",
    "test_info = (\"rl_arc_based\", \"one_for_each\", (0.33, 0.33, 0.34))\n",
    "for j in range(len(metric_names)):\n",
    "    metrics_instances_trans.append([(id_inst, \n",
    "                                     data_res_trans_rew[(id_inst, test_info)][j])\n",
    "                                        for id_inst in range(nb_instances)])\n",
    "    metrics_instances_trans[-1].sort(key = lambda x : -x[1])\n",
    "\n",
    "\n",
    "# Diplay the curves\n",
    "save_path = \"results/simulated/MFDS_vs_RL/results_test/trans_vs_no_trans\"\n",
    "colors = ['b', 'g', 'k', 'r', 'y', 'brown', 'purple', 'orange',  'gray', 'pink']\n",
    "markers = [\"o\", \"^\", \"s\", \"x\", \"*\", \"h\", \"p\", \"+\"]\n",
    "bins = 'auto'\n",
    "#bins = 100\n",
    "#force = None\n",
    "force = \"bootstrap\"\n",
    "with_pval_norm = False\n",
    "for i in range(len(metric_names)):\n",
    "    fig = plt.figure()\n",
    "\n",
    "    # Construct list of values of the metric for each instance\n",
    "    vals = [e[1] for e in metrics_instances_no_trans[i]]\n",
    "    # Process mean and reward\n",
    "    mean_ = np.mean(vals)\n",
    "    std_ = np.std(vals, ddof=1)\n",
    "    ci, pval_norm = mean_confidence_interval(vals, \n",
    "                                             confidence_normality_test = 0.95,\n",
    "                                             force = force,\n",
    "                                             confidence_level = 0.95)\n",
    "    name = \"no_transf_reward\"\n",
    "    conf_int_str = \"(\"+str(round(ci[0], 3))+\", \"+str(round(ci[1], 3))+\")\"\n",
    "    added_pval_norm = \" ^ \"+str(round(pval_norm,3)) if with_pval_norm else \"\"\n",
    "    label = name+\" \"+str(round(mean_,3))+\" \"+conf_int_str+added_pval_norm\n",
    "    # Plot\n",
    "    plt.scatter(list(range(len(metrics_instances_no_trans[i]))), \n",
    "                vals, \n",
    "                color = colors[0],\n",
    "                marker = markers[0], \n",
    "                alpha = 0.5, \n",
    "                label = label)\n",
    "    \n",
    "    # Construct list of values of the metric for each instance\n",
    "    vals = [e[1] for e in metrics_instances_trans[i]]\n",
    "    # Process mean and reward\n",
    "    mean_ = np.mean(vals)\n",
    "    std_ = np.std(vals, ddof=1)\n",
    "    ci, pval_norm = mean_confidence_interval(vals, \n",
    "                                             confidence_normality_test = 0.95,\n",
    "                                             force = force,\n",
    "                                             confidence_level = 0.95)\n",
    "    name = \"transf_reward\"\n",
    "    conf_int_str = \"(\"+str(round(ci[0], 3))+\", \"+str(round(ci[1], 3))+\")\"\n",
    "    added_pval_norm = \" ^ \"+str(round(pval_norm,3)) if with_pval_norm else \"\"\n",
    "    label = name+\" \"+str(round(mean_,3))+\" \"+conf_int_str+added_pval_norm\n",
    "    # Plot\n",
    "    plt.scatter(list(range(len(metrics_instances_no_trans[i]))), \n",
    "                vals, \n",
    "                color = colors[1],\n",
    "                marker = markers[1], \n",
    "                alpha = 0.5, \n",
    "                label = label)\n",
    "    \n",
    "    plt.xlabel(\"Instances\")\n",
    "    plt.ylabel(metric_names[i])\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid(visible = True)\n",
    "    if save_path:\n",
    "        fig.savefig(os.path.join(save_path, \n",
    "                                \"comp_transf_rew_\"+metric_names[i]), \n",
    "                                bbox_inches='tight', \n",
    "                                pad_inches=0)\n",
    "    plt.close(fig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba68c95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HADDAM\\AppData\\Local\\Temp\\ipykernel_22204\\1092848656.py:258: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  ax[k].legend(loc=\"upper right\")\n",
      "C:\\Users\\HADDAM\\AppData\\Local\\Temp\\ipykernel_22204\\1092848656.py:258: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  ax[k].legend(loc=\"upper right\")\n",
      "C:\\Users\\HADDAM\\AppData\\Local\\Temp\\ipykernel_22204\\1092848656.py:258: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  ax[k].legend(loc=\"upper right\")\n",
      "C:\\Users\\HADDAM\\AppData\\Local\\Temp\\ipykernel_22204\\1092848656.py:258: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  ax[k].legend(loc=\"upper right\")\n",
      "C:\\Users\\HADDAM\\AppData\\Local\\Temp\\ipykernel_22204\\1092848656.py:258: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  ax[k].legend(loc=\"upper right\")\n",
      "C:\\Users\\HADDAM\\AppData\\Local\\Temp\\ipykernel_22204\\1092848656.py:258: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  ax[k].legend(loc=\"upper right\")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Real : RL Heuristics vs baseline heuristics\n",
    "\"\"\"\n",
    "\n",
    "# Import pickle\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import pylab as py\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\HADDAM\\\\Documents\\\\Python Scripts\\\\multi_flow_decomp\\\\')\n",
    "\n",
    "from RL_methods.utils_stats import mean_confidence_interval, test_twoind_sample\n",
    "\n",
    "\n",
    "\n",
    "##########################  Baseline heuristics handling  ########################## \n",
    "\"\"\"\n",
    "{\"res_key_metadata\":res_key_metadata,\n",
    "\"res_value_metadata\":res_value_metadata, \n",
    "\"data\":deepcopy(dict(dict_results))}\n",
    "\"\"\"\n",
    "path_baseline_heuristics = \"results/real/MFDS_vs_RL/results_lieu_saint_baseline_heuristics.pickle\"\n",
    "with open(path_baseline_heuristics, \n",
    "          'rb') as handle:\n",
    "    file = pickle.load(handle)\n",
    "    test_names = file[\"res_key_metadata\"]\n",
    "    metric_names = file[\"res_value_metadata\"]\n",
    "    data_res_heur = file[\"data\"]\n",
    "\n",
    "nb_instances = len({id_inst for id_inst, _ in data_res_heur.keys()})\n",
    "\n",
    "\"\"\"\n",
    "dict_results[(ind_instance, test)] = (flow_val_res, flow_res, m_flow_res, \n",
    "                                        prop_fsupp, prop_sp, trans_func_res)\n",
    "test   -->    [\"min_time\", \n",
    "              \"max_capacity\", \n",
    "              \"trans_func\", \n",
    "              \"random\"]\n",
    "\"\"\"\n",
    "# Unzip to display (each metric on one list)\n",
    "heurs_name = \"min_time\"\n",
    "metrics_instances_heurs = []\n",
    "for j in range(len(metric_names)):\n",
    "    metrics_instances_heurs.append([(id_inst, \n",
    "                                     data_res_heur[(id_inst, heurs_name)][j]) \n",
    "                                        for id_inst in range(nb_instances)])\n",
    "    metrics_instances_heurs[-1].sort(key = lambda x : -x[1])\n",
    "\n",
    "\n",
    "\n",
    "##########################  RL heuristics handling  ##########################\n",
    "\"\"\"\n",
    "{\"res_key_metadata\":res_key_metadata,\n",
    "\"res_value_metadata\":res_value_metadata, \n",
    "\"data\":deepcopy(dict(dict_results))}\n",
    "\"\"\"\n",
    "path_rl_heuristics = \"results/real/MFDS_vs_RL/results_lieu_saint_rl_heuristics_lr=0,075.pickle\"\n",
    "with open(path_rl_heuristics, \n",
    "          'rb') as handle:\n",
    "    file = pickle.load(handle)\n",
    "    test_infos = file[\"res_key_metadata\"]\n",
    "    #metric_names = file[\"res_value_metadata\"]\n",
    "    data = file[\"data\"]\n",
    "\n",
    "\"\"\"\n",
    "dict_results[(ind_instance, test_infos)] = (flow_val_res, flow_res, \n",
    "                                            m_flow_res, prop_fsupp, \n",
    "                                            prop_sp, trans_func_res,\n",
    "                                            reward)\n",
    "test_infos = (path_type_selector, path_card_criteria, lr_rate, (coeff1, coeff2, coeff3))\n",
    "\"\"\"\n",
    "\n",
    "# The set of all the learning rates\n",
    "lr_rates = set(lr_rate for _, (_, _, lr_rate, _) in data)\n",
    "# The data associated to the arc based case\n",
    "data_arc_based = {(ind_instance, test_infos):data[(ind_instance, test_infos)]\n",
    "                                                for (ind_instance, test_infos) in data\n",
    "                                                    if test_infos[0] == \"rl_arc_based\" and\\\n",
    "                                                        test_infos[1] == \"one_for_each\" and\\\n",
    "                                                            test_infos[3][-1] != 0}\n",
    "\n",
    "# Calculate the best learning rate for the node_based case\n",
    "mean_rew_by_lr_rate = {np.mean([data_arc_based[(ind_instance, test_infos)][-1] \n",
    "                                    for (ind_instance, test_infos) in data_arc_based\n",
    "                                        if test_infos[2] == lr_rate]):lr_rate \n",
    "                                            for lr_rate in lr_rates}\n",
    "lr_rate_max_arc_based = mean_rew_by_lr_rate[max(mean_rew_by_lr_rate.keys())]\n",
    "\n",
    "\n",
    "# Filter to leave only the best value of the learning parameter for the arc based case\n",
    "data_res_rl_arc_based = {(ind_instance, \n",
    "                            (test_infos[0], \n",
    "                            test_infos[1], \n",
    "                            test_infos[3])):data_arc_based[(ind_instance, test_infos)]+(lr_rate_max_arc_based,)\n",
    "                                                for (ind_instance, test_infos) in data_arc_based\n",
    "                                                    if test_infos[2] == lr_rate_max_arc_based}\n",
    "\n",
    "# Unzip to display (each metric on one list)\n",
    "metrics_instances_rl_arc_based = []\n",
    "test_info = (\"rl_arc_based\", \"one_for_each\", (0.33, 0.33, 0.34))\n",
    "for j in range(len(metric_names)):\n",
    "    metrics_instances_rl_arc_based.append([(id_inst, \n",
    "                                            data_res_rl_arc_based[(id_inst, test_info)][j])\n",
    "                                                for id_inst in range(nb_instances)])\n",
    "    metrics_instances_rl_arc_based[-1].sort(key = lambda x : -x[1])\n",
    "\n",
    "# Diplay the curves\n",
    "save_path = \"results/real/MFDS_vs_RL/rl_vs_baseline\"\n",
    "colors = ['b', 'g', 'k', 'r', 'y', 'brown', 'purple', 'orange',  'gray', 'pink']\n",
    "markers = [\"o\", \"^\", \"s\", \"x\", \"*\", \"h\", \"p\", \"+\"]\n",
    "bins = 'auto'\n",
    "#bins = 100\n",
    "#force = None\n",
    "force = \"bootstrap\"\n",
    "with_pval_norm = False\n",
    "for i in range(len(metric_names)):\n",
    "    fig, fig3 = plt.figure(), plt.figure()\n",
    "    fig2, ax = plt.subplots(3, figsize=(8, 12))\n",
    "    \n",
    "    # Construct list of values of the metric for each instance\n",
    "    vals = [e[1] for e in metrics_instances_heurs[i]]\n",
    "    # Process mean and reward\n",
    "    mean_ = np.mean(vals)\n",
    "    std_ = np.std(vals, ddof=1)\n",
    "    ci, pval_norm = mean_confidence_interval(vals, \n",
    "                                             confidence_normality_test = 0.95,\n",
    "                                             force = force,\n",
    "                                             confidence_level = 0.95)\n",
    "    name = heurs_name\n",
    "    conf_int_str = \"(\"+str(round(ci[0], 3))+\", \"+str(round(ci[1], 3))+\")\"\n",
    "    added_pval_norm = \" ^ \"+str(round(pval_norm,3)) if with_pval_norm else \"\"\n",
    "    label = name+\" \"+str(round(mean_,3))+\" \"+conf_int_str+added_pval_norm\n",
    "    # Plot figure 1\n",
    "    py.figure(fig.number)\n",
    "    plt.scatter(list(range(len(vals))), \n",
    "                vals, \n",
    "                color = colors[0],\n",
    "                marker = markers[0], \n",
    "                alpha = 0.5, \n",
    "                label = label)\n",
    "    # Plot figure 2\n",
    "    py.figure(fig2.number)\n",
    "    sns.histplot(vals,\n",
    "                 color = colors[0], \n",
    "                 edgecolor = 'black', \n",
    "                 alpha = 0.5,\n",
    "                 bins = bins,\n",
    "                 kde = True,\n",
    "                 label = label,\n",
    "                 ax = ax[0])\n",
    "    \n",
    "    # Plot figure 3\n",
    "    py.figure(fig3.number)\n",
    "    box = plt.boxplot(vals, \n",
    "                      patch_artist = True,\n",
    "                      widths = 0.6,\n",
    "                      positions = [1])\n",
    "    box['boxes'][0].set_facecolor(colors[0])\n",
    "    box['medians'][0].set_color(\"white\")\n",
    "    # Plot qq\n",
    "    if metric_names[i] not in {\"prop_flow_support\", \"prop_shortest_paths\"}:\n",
    "        vals_centred = np.array([(v - mean_)/std_ for v in vals])\n",
    "        # Create a qq_plot\n",
    "        fig4 = sm.qqplot(vals_centred, line ='45')\n",
    "        # Save the qqplot\n",
    "        path = os.path.join(save_path,\n",
    "                            \"qq_plots\",\n",
    "                            \"comp_rl_vs_baseline_\"+metric_names[i]+\"_\"+label+\".png\")\n",
    "        py.figure(fig4.number)\n",
    "        py.savefig(path)\n",
    "        plt.close(fig4)\n",
    "    \n",
    "    \n",
    "    # Construct list of values of the metric for each instance\n",
    "    vals = [e[1] for e in metrics_instances_rl_arc_based[i]]\n",
    "    # Process mean and reward\n",
    "    mean_ = np.mean(vals)\n",
    "    std_ = np.std(vals, ddof=1)\n",
    "    ci, pval_norm = mean_confidence_interval(vals, \n",
    "                                             confidence_normality_test = 0.95,\n",
    "                                             force = force,\n",
    "                                             confidence_level = 0.95)\n",
    "    name = \"rl_arc_based\"\n",
    "    conf_int_str = \"(\"+str(round(ci[0], 3))+\", \"+str(round(ci[1], 3))+\")\"\n",
    "    added_pval_norm = \" ^ \"+str(round(pval_norm,3)) if with_pval_norm else \"\"\n",
    "    label = name+\" \"+str(round(mean_,3))+\" \"+conf_int_str+added_pval_norm\n",
    "    # Plot figure 1\n",
    "    py.figure(fig.number)\n",
    "    plt.scatter(list(range(len(vals))), \n",
    "                vals, \n",
    "                color = colors[2],\n",
    "                marker = markers[2], \n",
    "                alpha = 0.5, \n",
    "                label = label)\n",
    "    # Plot figure 2\n",
    "    py.figure(fig2.number)\n",
    "    sns.histplot(vals,\n",
    "                 color = colors[2], \n",
    "                 edgecolor = 'black', \n",
    "                 alpha = 0.5,\n",
    "                 bins = bins,\n",
    "                 kde = True,\n",
    "                 label = label,\n",
    "                 ax = ax[2])\n",
    "    \n",
    "    # Plot figure 3\n",
    "    py.figure(fig3.number)\n",
    "    box = plt.boxplot(vals, \n",
    "                      patch_artist = True,\n",
    "                      widths = 0.6,\n",
    "                      positions = [2])\n",
    "    box['boxes'][0].set_facecolor(colors[2])\n",
    "    box['medians'][0].set_color(\"white\")\n",
    "    # Plot qq\n",
    "    if metric_names[i] not in {\"prop_flow_support\", \"prop_shortest_paths\"}:\n",
    "        vals_centred = np.array([(v - mean_)/std_ for v in vals])\n",
    "        # Create a qq_plot\n",
    "        fig4 = sm.qqplot(vals_centred, line ='45')\n",
    "        # Save the qqplot\n",
    "        path = os.path.join(save_path,\n",
    "                            \"qq_plots\",\n",
    "                            \"comp_rl_vs_baseline_\"+metric_names[i]+\"_\"+label+\".png\")\n",
    "        py.figure(fig4.number)\n",
    "        py.savefig(path)\n",
    "        plt.close(fig4)\n",
    "    \n",
    "    py.figure(fig.number)\n",
    "    plt.xlabel(\"Instances\")\n",
    "    plt.ylabel(metric_names[i])\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid(visible = True)\n",
    "    if save_path:\n",
    "        fig.savefig(os.path.join(save_path, \n",
    "                                \"comp_rl_vs_baseline_\"+metric_names[i]), \n",
    "                                bbox_inches='tight', \n",
    "                                pad_inches=0)\n",
    "    plt.close(fig)\n",
    "\n",
    "    py.figure(fig2.number)\n",
    "    for k in range(2):\n",
    "        #ax[k].set_xlabel(metric_names[i])\n",
    "        #ax[k].set_xlabel(\"Frequency\")\n",
    "        ax[k].legend(loc=\"upper right\")\n",
    "        ax[k].grid(visible = True)\n",
    "    fig2.tight_layout()\n",
    "    if save_path:\n",
    "        fig2.savefig(os.path.join(save_path, \n",
    "                                  \"hist_comp_rl_vs_baseline_\"+metric_names[i]), \n",
    "                                  bbox_inches='tight', \n",
    "                                  pad_inches=0)\n",
    "    plt.close(fig2)\n",
    "\n",
    "\n",
    "    py.figure(fig3.number)\n",
    "    plt.xlabel(\"methods\")\n",
    "    plt.ylabel(\"ranges\")\n",
    "    #plt.legend(loc=\"upper right\")\n",
    "    plt.xticks([j+1 for j in range(2)], [heurs_name, \"rl_arc_based\"])\n",
    "    if save_path:\n",
    "        fig3.savefig(os.path.join(save_path, \n",
    "                                  \"box_comp_rl_vs_baseline_\"+metric_names[i]), \n",
    "                                  bbox_inches='tight', \n",
    "                                  pad_inches=0)\n",
    "    plt.close(fig3)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#clear_output(wait=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a38f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RL Heuristics\n",
    "\"\"\"\n",
    "\n",
    "# Import pickle\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "import sys\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\HADDAM\\\\Documents\\\\Python Scripts\\\\multi_flow_decomp\\\\')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "{\"res_key_metadata\":res_key_metadata,\n",
    "\"res_value_metadata\":res_value_metadata, \n",
    "\"data\":deepcopy(dict(dict_results))}\n",
    "\"\"\"\n",
    "with open(\"results/simulated/MFDS_vs_RL/results_test/results_rl_heuristics.pickle\", \n",
    "          'rb') as handle:\n",
    "    file = pickle.load(handle)\n",
    "    test_infos = file[\"res_key_metadata\"]\n",
    "    metric_names = file[\"res_value_metadata\"]\n",
    "    data = file[\"data\"]\n",
    "\n",
    "\"\"\"\n",
    "dict_results[(ind_instance, test_infos)] = (flow_val_res, flow_res, \n",
    "                                            m_flow_res, prop_fsupp, \n",
    "                                            prop_sp, trans_func_res,\n",
    "                                            reward)\n",
    "test_infos = (path_type_selector, path_card_criteria, lr_rate, (coeff1, coeff2, coeff3))\n",
    "\"\"\"\n",
    "\n",
    "# Filter to leave only the best value of the learning parameter\n",
    "data_res_rl, inserted = {}, set()\n",
    "for ind_instance, test_infos in data:\n",
    "    # Unpack\n",
    "    flow_val_res, flow_res, _, _, _, trans_func_res, reward = data[(ind_instance, test_infos)]\n",
    "    path_type_selector, path_card_criteria, lr_rate = test_infos[0], test_infos[1], test_infos[2]\n",
    "    coeff1, coeff2, coeff3 = test_infos[-1]\n",
    "    # The key to be (potentially) inserted\n",
    "    key_inserted = (ind_instance, (path_type_selector, \n",
    "                                   path_card_criteria, \n",
    "                                   (coeff1, coeff2, coeff3)))\n",
    "    # Insert key if not present else delete it\n",
    "    if key_inserted not in data_res_rl:\n",
    "        data_res_rl[key_inserted] = data[(ind_instance, test_infos)]+(lr_rate,)\n",
    "    elif reward > data_res_rl[key_inserted][-2]: \n",
    "        del data_res_rl[key_inserted]\n",
    "        data_res_rl[key_inserted] = data[(ind_instance, test_infos)]+(lr_rate,)\n",
    "\n",
    "\n",
    "# Unzip to display (each metric on one list)\n",
    "nb_instances = len({id_inst for id_inst, _ in data_res_rl.keys()})\n",
    "ls_test_infos = list(set([e[1] for e in data_res_rl.keys()]))\n",
    "dict_perfs_rl = {ls_test_infos[i]:[] for i in range(len(ls_test_infos))}\n",
    "for test_info in ls_test_infos:\n",
    "    metrics_instances = dict_perfs_rl[test_info]\n",
    "    for j in range(len(metric_names)):\n",
    "        metrics_instances.append([(id_inst, \n",
    "                                   data_res_rl[(id_inst, test_info)][j]) \n",
    "                                        for id_inst in range(nb_instances)])\n",
    "        metrics_instances[-1].sort(key = lambda x : -x[1])\n",
    "\n",
    "\n",
    "# Diplay the curves\n",
    "#!!! REECRITURE !!!\n",
    "save_path = \"results/simulated/MFDS_vs_RL/results_test/rl_heuristics/\"\n",
    "colors = ['b', 'g', 'k', 'r', 'y', 'brown', 'purple', 'orange',  'gray', 'pink']\n",
    "markers = [\"o\", \"^\", \"s\", \"x\", \"*\", \"h\", \"p\", \"+\"]\n",
    "show_std = False\n",
    "for i in range(len(metric_names)):\n",
    "    fig = plt.figure()\n",
    "    for j in range(len(ls_test_infos)):\n",
    "        # Unpack\n",
    "        test_info = ls_test_infos[j]\n",
    "        path_type_selector, path_card_criteria, (coeff1, coeff2, coeff3) = test_info\n",
    "        # Construct list of values of the metric for each instance\n",
    "        vals = [e[1] for e in dict_perfs_rl[test_info][i]]\n",
    "        # Process mean and reward\n",
    "        mean_ = np.mean(vals)\n",
    "        std_ = np.std(vals, ddof=1)\n",
    "        ci = stats.t.interval(confidence = 0.95, \n",
    "                              df = len(vals)-1, \n",
    "                              loc = mean_, \n",
    "                              scale = std_ / np.sqrt(len(vals)))\n",
    "        name = path_type_selector+\" \"+path_card_criteria+\" \"+\" c3=\"+str(coeff3)\n",
    "        conf_int_str = \"(\"+str(round(ci[0], 3))+\", \"+str(round(ci[1], 3))+\")\"\n",
    "        if show_std:\n",
    "            label = name+\" \"+str(round(mean_,3))+\"+/-\"+str(round(std_,3))+\" \"+conf_int_str\n",
    "        else:\n",
    "            label = name+\" \"+str(round(mean_,3))+\" \"+conf_int_str\n",
    "        plt.scatter(list(range(len(dict_perfs_rl[test_info][i]))), \n",
    "                    vals, \n",
    "                    color = colors[j],\n",
    "                    marker = markers[j], \n",
    "                    alpha = 0.5, \n",
    "                    label = label)\n",
    "    plt.xlabel(\"Instances\")\n",
    "    plt.ylabel(metric_names[i])\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid(visible = True)\n",
    "    if save_path:\n",
    "        fig.savefig(os.path.join(save_path, \n",
    "                                \"comp_general_rl_heurs_\"+metric_names[i]), \n",
    "                                bbox_inches='tight', \n",
    "                                pad_inches=0)\n",
    "    plt.close(fig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40653a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\HADDAM\\\\Documents\\\\Python Scripts\\\\multi_flow_decomp\\\\')\n",
    "\n",
    "# Test to see if ranking has meaning\n",
    "\n",
    "# mean and standard deviation\n",
    "mu, sigma = 0, 1.0 \n",
    "path = \"results/simulated/MFDS_vs_RL/test_sorted_curves/\"\n",
    "nb_generated = 100\n",
    "show_ = False\n",
    "for i in range(nb_generated):\n",
    "    # first dataset\n",
    "    dataset1 = list(np.random.normal(mu, sigma, 100))\n",
    "    dataset1.sort(reverse=True)\n",
    "\n",
    "    # second dataset\n",
    "    dataset2 = list(np.random.normal(mu, sigma, 100))\n",
    "    dataset2.sort(reverse=True)\n",
    "    fig = plt.figure()\n",
    "\n",
    "    plt.scatter(list(range(100)), \n",
    "                dataset1, \n",
    "                color = \"r\",\n",
    "                marker = \"^\", \n",
    "                alpha = 0.5, \n",
    "                label = \"dataset1\")\n",
    "\n",
    "    plt.scatter(list(range(100)), \n",
    "                dataset2, \n",
    "                color = \"b\",\n",
    "                marker = \"o\", \n",
    "                alpha = 0.5, \n",
    "                label = \"dataset2\")\n",
    "    \n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid(visible = True)\n",
    "\n",
    "    fig.savefig(os.path.join(path,\n",
    "                             str(i)), \n",
    "                             bbox_inches='tight', \n",
    "                             pad_inches=0)\n",
    "    plt.close(fig)\n",
    "    \n",
    "\n",
    "if show_: plt.show()\n",
    "\n",
    "clear_output(wait=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
